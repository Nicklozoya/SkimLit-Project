{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVnaVeqDWLY6"
      },
      "source": [
        "# Milestone Project 2: SkimLit\n",
        "\n",
        "The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n",
        "\n",
        "Were going to be replicating the paper https://arxiv.org/abs/1612.05251"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpwFIhxrMd6g"
      },
      "source": [
        "## Get data\n",
        "Since we'll be replicating the paper above (Pubmed 200k RCT), let's download the dataset they used. We can do so through the authors Github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W8zKJ9SNkc1"
      },
      "outputs": [],
      "source": [
        "# Get helper functions file\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixZo2F3YNv1P"
      },
      "outputs": [],
      "source": [
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys, calculate_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQayhlnJNA5t"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSXLVoq9NjHB"
      },
      "outputs": [],
      "source": [
        "# Check what files are in the PubMed_20k dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWMcZaL_N4nI"
      },
      "outputs": [],
      "source": [
        "# Start our experiments using the 20k dataset with numbers replaced by \"@\" sign\n",
        "data_dir = '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v34ERk24TAX4"
      },
      "outputs": [],
      "source": [
        "# Check all of the files name in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xc5-qA6TLVL"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDkFfS7UU4-B"
      },
      "outputs": [],
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  \"\"\"\n",
        "  Reads filename (a text filename) and returns the lines of text as a list.\n",
        "\n",
        "  Args:\n",
        "   filename: a string containing the target filepath\n",
        "\n",
        "   Returns:\n",
        "    A list of strings with one string per line from the target filename.\n",
        "  \"\"\"\n",
        "  with open(filename, 'r') as f:\n",
        "    return f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVZIuvriXToQ"
      },
      "outputs": [],
      "source": [
        "# Let's read in the training lines\n",
        "train_lines = get_lines(data_dir+'train.txt')\n",
        "train_lines[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31eNk5FOXb1Y"
      },
      "outputs": [],
      "source": [
        "len(train_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTuSrBfzZOPw"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(filename):\n",
        "    \"\"\"\n",
        "    Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "    Takes in filename, reads its contents and sorts through each line,\n",
        "    extracting things like the target label, the text of the sentence,\n",
        "    how many sentences are in the current abstract and what sentence number the target line is\n",
        "    \"\"\"\n",
        "    input_lines = get_lines(filename)  # get all lines from filename\n",
        "    abstract_lines = ''  # create an empty abstract\n",
        "    abstract_samples = []  # create an empty list of abstracts\n",
        "\n",
        "    for line in input_lines:\n",
        "        if line.startswith('###'):\n",
        "            abstract_id = line\n",
        "            abstract_lines = \"\"\n",
        "        elif line.isspace():\n",
        "            abstract_line_split = abstract_lines.splitlines()\n",
        "\n",
        "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "                line_data = {}\n",
        "                target_text_split = abstract_line.split('\\t')\n",
        "                line_data['target'] = target_text_split[0]\n",
        "                line_data['text'] = target_text_split[1].lower()\n",
        "                line_data['line_number'] = abstract_line_number\n",
        "                line_data['total_lines'] = len(abstract_line_split) - 1\n",
        "                abstract_samples.append(line_data)  # append the dictionary to the list\n",
        "\n",
        "        else:\n",
        "            abstract_lines += line\n",
        "\n",
        "    return abstract_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBHaLzXBdZ_w"
      },
      "outputs": [],
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text(data_dir + 'train.txt')\n",
        "val_samples = preprocess_text(data_dir + 'dev.txt')\n",
        "test_samples = preprocess_text(data_dir + 'test.txt')\n",
        "\n",
        "print(len(train_samples), len(val_samples), len(test_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BwPrqrPtt_0j"
      },
      "outputs": [],
      "source": [
        "# Check the first abstract of our training data\n",
        "train_samples[:16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVZ_Wdf6ugSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3XnSSE-vMqV"
      },
      "outputs": [],
      "source": [
        "# Distriubtions of labels\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ax3h-cvwe16"
      },
      "outputs": [],
      "source": [
        "# Let's check the length of different lines\n",
        "train_df.total_lines.plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1RjKEeywrSU"
      },
      "source": [
        "### Get list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbT042qxw3c6"
      },
      "outputs": [],
      "source": [
        "# Convert abstract text lines into lists\n",
        "train_sentences = train_df['text'].tolist()\n",
        "val_sentences = val_df['text'].tolist()\n",
        "test_sentences = test_df['text'].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5WyZNLlxSLL"
      },
      "outputs": [],
      "source": [
        "# View the first 10 lines of training sentences\n",
        "train_sentences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IdybWvGxYiI"
      },
      "source": [
        "## Make numberic labels (ML models require numeric labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ipua11zyFSD"
      },
      "outputs": [],
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1,1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df['target'].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df['target'].to_numpy().reshape(-1,1))\n",
        "\n",
        "train_labels_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9fFNqgVyoZk"
      },
      "outputs": [],
      "source": [
        "# Extract labels ('target' columns) and encode them into integerts\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df['target'].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df['target'].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmFAEMg5Qvwe"
      },
      "outputs": [],
      "source": [
        "# Get class names and number of classes from LabelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes,class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iJf5XufJRUkP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model_0 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "model_0.fit(train_sentences,train_labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0GfWFLcSxeM"
      },
      "outputs": [],
      "source": [
        "model_0_preds = model_0.predict(val_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrU9wINbUvX4"
      },
      "outputs": [],
      "source": [
        "model_0.score(val_sentences,val_labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLbNpDJqTJi1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_labels_encoded, model_0_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra5eSsCATVtp"
      },
      "outputs": [],
      "source": [
        "model_0_results = calculate_results(val_labels_encoded,model_0_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AOpt4QLWvWC"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ent0L7bfDw0_"
      },
      "outputs": [],
      "source": [
        "# How long is each sentence on average?\n",
        "import numpy as np\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UiUi4UnE7NK"
      },
      "outputs": [],
      "source": [
        "# Whats the distrubution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK8rTW80Fnv7"
      },
      "outputs": [],
      "source": [
        "# How long of a sentence length covers 95% of examples?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TsvkTmRIa-L"
      },
      "outputs": [],
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EIKUvrnJIo0"
      },
      "outputs": [],
      "source": [
        "# Take the TensorsliceDatasets and turn them into prefetch datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eevJ8h4jyLF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Input, Conv1D, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define constants\n",
        "max_tokens = 68000\n",
        "output_seq_len = 55  # From your percentile calculation\n",
        "num_classes = 5  # Number of target classes\n",
        "\n",
        "# Prepare training sentences\n",
        "train_sentences = train_df['text'].to_list()  # Ensure this is a list of strings\n",
        "train_sentences = np.array(train_sentences)  # Convert to numpy array\n",
        "\n",
        "# Define and adapt the TextVectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=output_seq_len\n",
        ")\n",
        "vectorize_layer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHY7xVodiK4K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the vocabulary size after adaptation\n",
        "rct_20k_text_vocab = vectorize_layer.get_vocabulary()\n",
        "vocab_size = len(rct_20k_text_vocab)\n",
        "print(f\"Vocabulary size after adaptation: {vocab_size}\")  # Debug step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bnkpg68X9gE"
      },
      "outputs": [],
      "source": [
        "# Define the embedding layer with the correct input_dim\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,  # Use the adapted vocabulary size\n",
        "    output_dim=128,\n",
        "    mask_zero=True,\n",
        "    name='token_embedding'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUFOVg0Caxbk"
      },
      "outputs": [],
      "source": [
        "# Adapt the vectorization layer to your training data\n",
        "import numpy as np\n",
        "train_sentences = np.array(train_sentences)\n",
        "val_sentences = np.array(val_sentences)\n",
        "vectorize_layer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBYTpqVUCqCp"
      },
      "outputs": [],
      "source": [
        "# Convert the text data to numerical format using the TextVectorization layer\n",
        "train_sentences_num = vectorize_layer(train_sentences)\n",
        "val_sentences_num = vectorize_layer(val_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymePybD-Y6pq"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = vectorize_layer(inputs)\n",
        "token_embedding = embedding_layer(text_vectors)\n",
        "x = Conv1D(64, 5, padding='same', activation='relu')(token_embedding)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "model_1 = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Check the model summary\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-KFBvqIZwrd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model\n",
        "history_model_1 = model_1.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "    epochs=3,\n",
        "    validation_data=valid_dataset,\n",
        "    validation_steps=int(0.1 * len(valid_dataset))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO3wcAjoZ8Q7"
      },
      "outputs": [],
      "source": [
        "model_1.evaluate(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqhtjS0EyW2E"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV3oUfiKyj6l"
      },
      "outputs": [],
      "source": [
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "model_1_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIVR1xxNy6Bi"
      },
      "outputs": [],
      "source": [
        "model_1_results = calculate_results(val_labels_encoded,\n",
        "                                    model_1_preds)\n",
        "model_1_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXPDpzb3zGvF"
      },
      "outputs": [],
      "source": [
        "model_0_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLHpmJLkzXzH"
      },
      "source": [
        "## Model 2: Featue extraction with pretrained token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYnWTthy6j6Q"
      },
      "outputs": [],
      "source": [
        "# Download pretrained TensofrFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "# Load the Universal Sentence Encoder\n",
        "use_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk0VLw5vazy3"
      },
      "outputs": [],
      "source": [
        "# Define a custom Keras layer for the USE\n",
        "class USEEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(USEEmbedding, self).__init__(**kwargs)\n",
        "        self.use = use_layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.use(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9626jbHbS0ij"
      },
      "outputs": [],
      "source": [
        "# Define the model using the Functional API\n",
        "input_layer = tf.keras.Input(shape=[], dtype=tf.string)\n",
        "embedding = USEEmbedding()(input_layer)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(embedding)\n",
        "outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the model\n",
        "model_2 = tf.keras.Model(input_layer, outputs)\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Test it\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkCYY5MJngF9"
      },
      "outputs": [],
      "source": [
        "history_model_2 = model_2.fit(train_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1 * len(valid_dataset))\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymy7qqQDr1N2"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the whole validation dataset\n",
        "model_2.evaluate(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHNUHB2osLER"
      },
      "outputs": [],
      "source": [
        "# Make predictions with feature extraction model\n",
        "model_2_pred_probs = model_2.predict(valid_dataset)\n",
        "model_2_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-k-LJBssWdx"
      },
      "outputs": [],
      "source": [
        "# Convert the prediction probabilities found with feature extraction model to labels\n",
        "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
        "model_2_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlqKWt9Qsn7U"
      },
      "outputs": [],
      "source": [
        "# Calculate results from TF Hub pretrained embeddings results on val set\n",
        "model_2_results = calculate_results(val_labels_encoded,\n",
        "                                    model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBf0bIh6t_wM"
      },
      "source": [
        "## Model 3: Conv1D with character embeddings\n",
        "The paper that we're replicating states they used a combination of token and character embeddings.\n",
        "\n",
        "previously we made token embeddings but we'll need to do similar steps for characters if we want to use char-level embedings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhor2SlCvmbP"
      },
      "source": [
        "### Create a character level tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwVZcH3zwHz6"
      },
      "outputs": [],
      "source": [
        "train_sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyIaY0DNwKpx"
      },
      "outputs": [],
      "source": [
        "# Make function to split sentences into charactesr\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVVGbwTiwTGE"
      },
      "outputs": [],
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejtpk6FUwzSd"
      },
      "outputs": [],
      "source": [
        "# What's the average character length?\n",
        "chars_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(chars_lens)\n",
        "mean_char_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdh3tsI0xqeK"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of our sequences at a character-level\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(chars_lens, bins=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FnrARZJyD0k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Find character length that covers 95% of sequences\n",
        "output_seq_len = int(np.percentile(chars_lens, 95))\n",
        "output_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaXqufRYyZZz"
      },
      "outputs": [],
      "source": [
        "# Get all keboard characters\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Dx6hlW26vg"
      },
      "outputs": [],
      "source": [
        "# Create char-level token vecotrizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
        "char_vectorizer = TextVectorization(max_tokens= NUM_CHAR_TOKENS,\n",
        "                                    output_sequence_length = output_seq_len,\n",
        "                                    standardize='lower_and_strip_punctuation',\n",
        "                                    name='char_vectorizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QerCrjfW31i2"
      },
      "outputs": [],
      "source": [
        "# Adapt character vectorizer to training cahracters\n",
        "char_vectorizer.adapt(train_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbyNw7grJeVY"
      },
      "outputs": [],
      "source": [
        "char_vocab = char_vectorizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZbAWz6Jr3Nw"
      },
      "outputs": [],
      "source": [
        "char_embed = tf.keras.layers.Embedding(input_dim=len(char_vocab),\n",
        "                              output_dim=25,\n",
        "                              mask_zero=True,\n",
        "                              name='char_embed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gaH6Tzx38Z1"
      },
      "outputs": [],
      "source": [
        "# Check character voacab stats\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
        "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
        "print(f\"5 least common characters: {char_vocab[-5:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6du0n-yCFt8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "random_train_chars = random.choice(train_chars)\n",
        "print(f\"Charified text: \\n {random_train_chars}\")\n",
        "print(f\"Length of random sentence {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f\"Vectorized chars: \\n {vectorized_chars}\")\n",
        "print(f\"Length of vectorized chars: \\n {len(vectorized_chars[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEtHhoOMmw5b"
      },
      "source": [
        "### Building a Conv1D to fit on character embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26IPNz32raky"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "char_vectors = char_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = tf.keras.layers.Conv1D(64,5, padding='same', activation='relu')(char_embeddings)\n",
        "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "model_3 = tf.keras.Model(inputs,outputs, name='model_3_conv1d_char_embeddings')\n",
        "\n",
        "model_3.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D109xvV8s-XH"
      },
      "outputs": [],
      "source": [
        "# Create char level datasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lbT0XUHypqm"
      },
      "outputs": [],
      "source": [
        "train_char_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdG1CMSVyEp5"
      },
      "outputs": [],
      "source": [
        "# fit the model on chars only\n",
        "model_3_history = model_3.fit(train_char_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1 * (len(train_char_dataset))),\n",
        "                              validation_data=val_char_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxXZWmgWyd1m"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
        "model_3_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNrqqDbOzyqF"
      },
      "outputs": [],
      "source": [
        "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
        "model_3_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFFXbvmHz9Ev"
      },
      "outputs": [],
      "source": [
        "model_3_results = calculate_results(val_labels_encoded,model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYbPeLAC0Enp"
      },
      "source": [
        "## Model 4: Combining pretrained token embeddings + character embeddings (hybrid approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UgvuY-iGJxl"
      },
      "outputs": [],
      "source": [
        "# Download pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainable=False,\n",
        "                                        name=\"universal_sentence_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5_K0InCKbO7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "# Wrap the TensorFlow Hub layer in a custom Keras layer\n",
        "@register_keras_serializable()  # Register the custom class for serialization\n",
        "class USEEmbedding(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(USEEmbedding, self).__init__(**kwargs)\n",
        "        self.tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False, name=\"universal_sentence_encoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.tf_hub_embedding_layer(inputs)\n",
        "\n",
        "    def get_config(self):  # Define get_config for serialization\n",
        "        config = super(USEEmbedding, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
        "# Use the custom layer to apply the USE embeddings\n",
        "token_embeddings = USEEmbedding()(token_inputs)\n",
        "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_output)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,\n",
        "                                                                  char_model.output])\n",
        "\n",
        "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# 5. Construct model with char and token inputs\n",
        "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_4_token_and_char_embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfeaYGKyrVij"
      },
      "outputs": [],
      "source": [
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qn1yCZgSrkia"
      },
      "outputs": [],
      "source": [
        "# Plot hybrid token and character model\n",
        "from keras.utils import plot_model\n",
        "plot_model(model_4,show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgq_u2_AtC--"
      },
      "outputs": [],
      "source": [
        "# Compile token char model\n",
        "model_4.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y15jVE05viWb"
      },
      "source": [
        "### Combinding token and character data into a tf.Data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2BdGXFzwFMv"
      },
      "outputs": [],
      "source": [
        "# Combine chars and tokens into a dataset\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Repeat same steps validation data\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
        "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1VArcKFx0UD"
      },
      "outputs": [],
      "source": [
        "train_char_token_dataset, val_char_token_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMPKR5ggx_GK"
      },
      "source": [
        "### Fitting a model on token and character-level sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqN0nFMOCLgx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fit the model on tokens and chars\n",
        "model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
        "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_token_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_token_dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jAes0mZQVju"
      },
      "outputs": [],
      "source": [
        "model_4.evaluate(val_char_token_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyQBsaMMS1Va"
      },
      "outputs": [],
      "source": [
        "model_4_pred_prods = model_4.predict(val_char_token_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acBsQXfaTDFh"
      },
      "outputs": [],
      "source": [
        "model_4_preds = tf.argmax(model_4_pred_prods, axis=1)\n",
        "model_4_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzjCTUIATRHW"
      },
      "outputs": [],
      "source": [
        "model_4_results = calculate_results(val_labels_encoded,\n",
        "                                    model_4_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOLLWkNPTlln"
      },
      "outputs": [],
      "source": [
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs7sDeZ5Tn8q"
      },
      "source": [
        "## Model 5: Transfer learning with pretrained token embeddings + character embeddings + positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTBzgwnTWE8l"
      },
      "source": [
        "### Create positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_WGPJkOX8Bg"
      },
      "outputs": [],
      "source": [
        "train_df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CV_mLQIqXlkH"
      },
      "outputs": [],
      "source": [
        "train_df['line_number'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4GjIs2cXwto"
      },
      "outputs": [],
      "source": [
        "train_df.line_number.plot.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-pMaDy2YPjW"
      },
      "outputs": [],
      "source": [
        "train_line_numbers_one_hot = tf.one_hot(train_df['line_number'].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df['line_number'].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df['line_number'].to_numpy(), depth=15)\n",
        "train_line_numbers_one_hot, train_line_numbers_one_hot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEYeuKkBYyiV"
      },
      "outputs": [],
      "source": [
        "train_df['total_lines'].plot.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQwhoclza48x"
      },
      "outputs": [],
      "source": [
        "np.percentile(train_df.total_lines, 98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSFCX1hOZwT_"
      },
      "outputs": [],
      "source": [
        "train_total_lines_one_hot = tf.one_hot(train_df['total_lines'].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df['total_lines'].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df['total_lines'].to_numpy(), depth=20)\n",
        "train_total_lines_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBFu1p49aG-I"
      },
      "source": [
        "### Building a tribrid embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-ItLHM0zEDI"
      },
      "outputs": [],
      "source": [
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype='string', name='token_inputs')\n",
        "token_embeddings = USEEmbedding()(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
        "# Reshape the output to be 3D for GlobalAveragePooling1D\n",
        "token_outputs = layers.Reshape((1, 128))(token_outputs)  # Reshape to (batch_size, 1, features)\n",
        "# Apply GlobalAveragePooling1D to reduce dimensionality\n",
        "token_outputs = layers.GlobalAveragePooling1D()(token_outputs)\n",
        "token_model = tf.keras.Model(token_inputs,\n",
        "                             token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape=(1,), dtype='string', name='char_inputs')\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(char_inputs,\n",
        "                            char_bi_lstm)\n",
        "\n",
        "# 3 Line numbers model\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name='line_number_input')\n",
        "x = layers.Dense(32, activation='relu')(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(line_number_inputs,\n",
        "                                   x)\n",
        "# 4. Total lines model\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name='total_lines_input')\n",
        "y = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
        "total_line_model = tf.keras.Model(total_lines_inputs,\n",
        "                                  y)\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name='char_token_hybrid_embedding')([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "z = layers.Dense(256, activation='relu')(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embeddings with combined token and char embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name='char_token_positional_embedding')([line_number_model.output,\n",
        "                                                                                 total_line_model.output,\n",
        "                                                                                 z])\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(5, activation='softmax', name='output_layer')(tribrid_embeddings)\n",
        "\n",
        "# 8. Put together model\n",
        "model_5 = tf.keras.Model([line_number_model.input,\n",
        "                          total_line_model.input,\n",
        "                          token_model.input,\n",
        "                          char_model.input],\n",
        "                         output_layer, name='tribrid_embedings_model_5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycMSJ04Eipi_"
      },
      "outputs": [],
      "source": [
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZGVwMMl_3iL6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTOO25Cf4ClY"
      },
      "outputs": [],
      "source": [
        "# Explicitly set steps_per_execution to 1\n",
        "model_5.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        "    steps_per_execution=1  # Force single-step execution\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfeV2zeh7tT8"
      },
      "outputs": [],
      "source": [
        "# Convert lists to tensors and reshape if necessary\n",
        "if isinstance(train_chars, list):\n",
        "    train_chars = tf.constant(train_chars)\n",
        "if isinstance(val_chars, list):\n",
        "    val_chars = tf.constant(val_chars)\n",
        "\n",
        "if len(train_chars.shape) == 1:\n",
        "    train_chars = tf.expand_dims(train_chars, axis=-1)  # Shape: (180040, 1)\n",
        "if len(val_chars.shape) == 1:\n",
        "    val_chars = tf.expand_dims(val_chars, axis=-1)\n",
        "\n",
        "# 1. Create individual datasets for each of the 4 input tensors\n",
        "train_line_numbers_dataset = tf.data.Dataset.from_tensor_slices(train_line_numbers_one_hot)  # (180040, 15), float32\n",
        "train_total_lines_dataset = tf.data.Dataset.from_tensor_slices(train_total_lines_one_hot)    # (180040, 20), float32\n",
        "train_sentences_dataset = tf.data.Dataset.from_tensor_slices(train_sentences)                # (180040,), string\n",
        "train_chars_dataset = tf.data.Dataset.from_tensor_slices(train_chars)                        # (180040, 1), string\n",
        "\n",
        "# 2. Create the labels dataset\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)              # (180040, 5), float64\n",
        "\n",
        "# 3. Zip the 4 input datasets into a single tuple, then zip with labels\n",
        "train_inputs_dataset = tf.data.Dataset.zipmeant = tf.data.Dataset.zip((\n",
        "    train_line_numbers_dataset,\n",
        "    train_total_lines_dataset,\n",
        "    train_sentences_dataset,\n",
        "    train_chars_dataset\n",
        "))\n",
        "train_char_token_pos_dataset = tf.data.Dataset.zip((train_inputs_dataset, train_labels_dataset))\n",
        "\n",
        "# 4. Batch and prefetch\n",
        "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation dataset\n",
        "val_line_numbers_dataset = tf.data.Dataset.from_tensor_slices(val_line_numbers_one_hot)\n",
        "val_total_lines_dataset = tf.data.Dataset.from_tensor_slices(val_total_lines_one_hot)\n",
        "val_sentences_dataset = tf.data.Dataset.from_tensor_slices(val_sentences)\n",
        "val_chars_dataset = tf.data.Dataset.from_tensor_slices(val_chars)\n",
        "val_labels_dataset = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "\n",
        "val_inputs_dataset = tf.data.Dataset.zip((\n",
        "    val_line_numbers_dataset,\n",
        "    val_total_lines_dataset,\n",
        "    val_sentences_dataset,\n",
        "    val_chars_dataset\n",
        "))\n",
        "val_char_token_pos_dataset = tf.data.Dataset.zip((val_inputs_dataset, val_labels_dataset))\n",
        "\n",
        "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (inputs, labels) in train_char_token_pos_dataset.take(1):\n",
        "    print(\"Input shapes:\")\n",
        "    for i, inp in enumerate(inputs):\n",
        "        print(f\"Input {i+1}: {inp.shape}, {inp.dtype}\")\n",
        "    print(f\"Labels: {labels.shape}, {labels.dtype}\")"
      ],
      "metadata": {
        "id": "wHi7anC7anY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfnTASYsRHUA"
      },
      "outputs": [],
      "source": [
        "history_model_6 = model_5.fit(\n",
        "    train_char_token_pos_dataset,\n",
        "    epochs=3,\n",
        "    steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
        "    validation_data=val_char_token_pos_dataset,\n",
        "    validation_steps=int(0.1 * len(val_char_token_pos_dataset))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjyxWgIvR1wx"
      },
      "outputs": [],
      "source": [
        "model_5_pred_probs = model_5.predict(val_char_token_pos_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "model_5_preds"
      ],
      "metadata": {
        "id": "GJN_foI9eqif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_results = calculate_results(val_labels_encoded,\n",
        "                                    model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "JJ5skanZexim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results into a datafram\n",
        "all_model_results = pd.DataFrame({'model_0_baseline': model_0_results,\n",
        "                                 'model_1_custom_token_embedding': model_1_results,\n",
        "                                 'model_2_pretrained_token_embedding': model_2_results,\n",
        "                                 'model_3_custom_char_embedding':model_3_results,\n",
        "                                 'model_4_hybrid+char_token_embedding': model_4_results,\n",
        "                                 'model_5_pos_char_token_embedding': model_5_results}).T\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "ORtf4VWMfUGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the accuracy to same scale as other metrics\n",
        "all_model_results['accuracy'] = all_model_results['accuracy']/100"
      ],
      "metadata": {
        "id": "Zd_Uk5XGfzJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_model_results.plot(kind='bar', figsize=(12, 6), width=0.8)\n",
        "plt.title('Model Performance Comparison (F1-Score and More)')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Metrics')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8SsNH3JgtSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the model results by f1-scores\n",
        "all_model_results.sort_values('f1', ascending=True)['f1'].plot(kind='bar', figsize=(10,7), ylabel='F1-score')"
      ],
      "metadata": {
        "id": "txjT02I2g2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('skimlit_tribrid', exist_ok=True)\n",
        "\n",
        "model_5.save('skimlit_tribrid/model_5.keras')"
      ],
      "metadata": {
        "id": "fDZVAmUDhhCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('skimlit_tribrid/model_5.keras')"
      ],
      "metadata": {
        "id": "dH142IDHiNow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_pred_probs = loaded_model.predict(val_char_token_pos_dataset)\n",
        "loaded_preds = tf.argmax(loaded_pred_probs, axis=1)\n",
        "loaded_preds"
      ],
      "metadata": {
        "id": "Y0Vy4cghik-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_results = calculate_results(val_labels_encoded,\n",
        "                                         loaded_preds)\n",
        "loaded_model_results"
      ],
      "metadata": {
        "id": "uBFH8XDYzRSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F00YWN4P0sIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}